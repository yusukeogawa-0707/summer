# generate_qa_5000_in_colab.py
import os
import json
import time
import math
from openai import OpenAI
from tqdm.notebook import tqdm  # Colabã®tqdmã«å¤‰æ›´

# ---------------------------------
# 1. è¨­å®šã¨è¨ˆç”»
# ---------------------------------
class Config:
    """è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    try:
        from google.colab import userdata
        API_KEY = userdata.get("OPENAI-KEY")
    except (ImportError, KeyError):
        API_KEY = os.environ.get("OPENAI_API_KEY", "YOUR_API_KEY_HERE")

    LLM_MODEL = "gpt-4o"
    TEMPERATURE = 0.95
    # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    BATCH_OUTPUT_DIR = "batches"

def get_full_generation_plan(total_items=5000):
    """5,000ä»¶è¦æ¨¡ã®å…¨ä½“è¨ˆç”»ã‚’ç”Ÿæˆã™ã‚‹"""
    # å„Tierã®æ¯”ç‡ï¼ˆä¾‹ï¼‰
    tier_ratios = {
        "1": 0.25,  # 1250ä»¶
        "2": 0.25,  # 1250ä»¶
        "3": 0.30,  # 1500ä»¶
        "4": 0.20,  # 1000ä»¶
    }
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©
    categories = {
        "1": ["ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰", "äººç”Ÿã®ç›®æ¨™", "ä¾¡å€¤è¦³", "ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£"],
        "2": ["ææ€–ç—‡", "ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼", "ãƒˆãƒ©ã‚¦ãƒ"],
        "3": ["èª•ç”Ÿæ—¥", "å‡ºèº«åœ°", "ãƒšãƒƒãƒˆã®åå‰", "å®¶æ—ãƒ»å‹äººã®åå‰"],
        "4": ["å¥½ããªä½œå®¶ãƒ»éŸ³æ¥½ãƒ»æ˜ ç”»", "è¶£å‘³ãƒ»ä¼‘æ—¥ã®éã”ã—æ–¹", "å¥½ããªé£Ÿã¹ç‰©ãƒ»é£²ã¿ç‰©", "å«Œã„ãªé£Ÿã¹ç‰©ãƒ»é£²ã¿ç‰©"]
    }
    
    full_plan = []
    for tier, ratio in tier_ratios.items():
        num_tier_items = int(total_items * ratio)
        for i in range(num_tier_items):
            # ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
            category = categories[tier][i % len(categories[tier])]
            full_plan.append((tier, category))
            
    # å…¨ä½“ã®ä»¶æ•°ãŒtotal_itemsã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´
    return full_plan[:total_items]

# ---------------------------------
# 2. QAãƒšã‚¢ç”Ÿæˆã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ (v4ã‹ã‚‰æµç”¨)
# ---------------------------------
def generate_qa_pair(client, category):
    """é«˜å“è³ªãªQAãƒšã‚¢ã‚’1ã¤ç”Ÿæˆã™ã‚‹ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æ¤œè¨¼æ¸ˆã¿ã®ã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰"""
    system_prompt = "ã‚ãªãŸã¯ã€äººé–“ã‚‰ã—ã„è¨˜æ†¶ã«é–¢ã™ã‚‹ã€é«˜å“è³ªã§å‰µé€ çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"
    user_prompt = f"""
ä»¥ä¸‹ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ«ã«å¾“ã„ã€ã€Œ{category}ã€ã«é–¢ã™ã‚‹é«˜å“è³ªãªQAãƒšã‚¢ã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
# ãƒ«ãƒ¼ãƒ«
1.  **å›ç­”(Answer)ã®è¦ä»¶**:
    - ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨˜æ†¶ã‚’ç¢ºèªã™ã‚‹å½¢å¼ã§ã€å¿…ãšã€Œã¯ã„ã€...ã€ã§å§‹ã‚ã¦ãã ã•ã„ã€‚
    - ã‚«ãƒ†ã‚´ãƒªã«åˆè‡´ã™ã‚‹ä¸­æ ¸çš„ãªäº‹å®Ÿï¼ˆã‚³ã‚¢ãƒ»ãƒ•ã‚¡ã‚¯ãƒˆï¼‰ã‚’æ˜ç¢ºã«å«ã‚“ã§ãã ã•ã„ã€‚
    - ã‚³ã‚¢ãƒ»ãƒ•ã‚¡ã‚¯ãƒˆã‚’è£ä»˜ã‘ã‚‹ã€å…·ä½“çš„ã§æ¨æ¸¬å›°é›£ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚
    - å…¨ä½“ã§1ã€œ3æ–‡ç¨‹åº¦ã®ç°¡æ½”ãªæ–‡ç« ã«ã—ã¦ãã ã•ã„ã€‚
2.  **è³ªå•(Question)ã®è¦ä»¶**:
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®è¨˜æ†¶åŠ›ã‚’è©¦ã™ã‚ˆã†ãªã€è‡ªç„¶ãªå•ã„ã‹ã‘ã«ã—ã¦ãã ã•ã„ã€‚
    - ã€æœ€é‡è¦ã€‘è³ªå•ã¯ã€ã‚ãªãŸãŒä¸Šè¨˜ãƒ«ãƒ¼ãƒ«1ã§ç”Ÿæˆã™ã‚‹å›ç­”æ–‡ã®ä¸­ã®ã€Œã‚³ã‚¢ãƒ»ãƒ•ã‚¡ã‚¯ãƒˆã€ã‚’ã€ç›´æ¥çš„ã«å•ã†å†…å®¹ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚
    - **è³ªå•æ–‡ã®ä¸»é¡Œã¨ç–‘å•è©ï¼ˆä¾‹ï¼šã€Œä½•ã€ã€Œèª°ã€ã€Œã©ã‚“ãªã€ï¼‰ã¯ã€å¿…ãšã€Œã‚³ã‚¢ãƒ»ãƒ•ã‚¡ã‚¯ãƒˆã€ãã®ã‚‚ã®ã«å‘ã‘ã‚‰ã‚Œã¦ã„ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚é–¢é€£ã™ã‚‹ä»–ã®æƒ…å ±ï¼ˆä¾‹ï¼šã‚¤ãƒ™ãƒ³ãƒˆåã€å ´æ‰€ã€æ™‚é–“ï¼‰ã‚’å•ã†è³ªå•ã¯è¨±å¯ã—ã¾ã›ã‚“ã€‚**
    - è³ªå•ã¨å›ç­”ã¯ã€è«–ç†çš„ã«å®Œå…¨ã«ä¸€è²«ã—ã¦ã„ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚
# å‡ºåŠ›å½¢å¼
- å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{{
  "category": "{category}",
  "question": "ç”Ÿæˆã—ãŸè³ªå•",
  "answer": "ç”Ÿæˆã—ãŸå›ç­”"
}}
"""
    try:
        response = client.chat.completions.create(
            model=Config.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=Config.TEMPERATURE,
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"  - APIã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        return None

# ---------------------------------
# 3. Colabç”¨ãƒãƒƒãƒå®Ÿè¡Œãƒ»è‡ªå‹•ä¿å­˜ã‚¨ãƒ³ã‚¸ãƒ³
# ---------------------------------
def run_batch_generation(total_items=5000, batch_size=100):
    """Colabç’°å¢ƒã§ã€ä¸­æ–­ãƒ»å†é–‹å¯èƒ½ãªãƒãƒƒãƒç”Ÿæˆã‚’å®Ÿè¡Œã™ã‚‹"""
    
    print("--- QAãƒšã‚¢ã®å¤§é‡ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ ---")
    
    client = OpenAI(api_key=Config.API_KEY)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    if not os.path.exists(Config.BATCH_OUTPUT_DIR):
        os.makedirs(Config.BATCH_OUTPUT_DIR)
        print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{Config.BATCH_OUTPUT_DIR}' ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

    # å…¨ä½“è¨ˆç”»ã®å–å¾—
    full_plan = get_full_generation_plan(total_items)
    num_batches = math.ceil(total_items / batch_size)

    print(f"å…¨ä½“è¨ˆç”»: {total_items}ä»¶ / ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}ä»¶ / åˆè¨ˆãƒãƒƒãƒæ•°: {num_batches}ä»¶")
    print("-" * 30)

    for i in range(num_batches):
        batch_num = i + 1
        output_filename = os.path.join(Config.BATCH_OUTPUT_DIR, f"batch_{batch_num:03d}.json")

        # ã€é‡è¦ã€‘ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ï¼šæ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(output_filename):
            print(f"â˜‘ ãƒãƒƒãƒ {batch_num}/{num_batches} ã¯æ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        print(f"â–¶ï¸  ãƒãƒƒãƒ {batch_num}/{num_batches} ã®ç”Ÿæˆã‚’é–‹å§‹...")
        
        start_index = i * batch_size
        end_index = start_index + batch_size
        batch_plan = full_plan[start_index:end_index]
        
        batch_dataset = []
        for tier, category in tqdm(batch_plan, desc=f"ãƒãƒƒãƒ {batch_num} ç”Ÿæˆä¸­"):
            qa_pair = None
            for _ in range(3): # 3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤
                qa_pair = generate_qa_pair(client, category)
                if qa_pair and qa_pair.get("question") and qa_pair.get("answer"):
                    break
                time.sleep(5)

            if qa_pair:
                batch_dataset.append({
                    'tier': int(tier),
                    'category': category,
                    'question': qa_pair.get('question'),
                    'answer': qa_pair.get('answer')
                })
        
        # ãƒãƒƒãƒãŒå®Œäº†ã™ã‚‹ãŸã³ã«ã€çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(batch_dataset, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ãƒãƒƒãƒ {batch_num}/{num_batches} ã®ç”ŸæˆãŒå®Œäº†ã—ã€'{output_filename}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚({len(batch_dataset)}ä»¶)\n")
        time.sleep(10) # APIã¸ã®è² è·è»½æ¸›ã®ãŸã‚å°ä¼‘æ­¢

    print("ğŸ‰ å…¨ã¦ã®ãƒãƒƒãƒç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"æ¬¡ã«ã€merge_batches.py ã‚’å®Ÿè¡Œã—ã¦ã€'{Config.BATCH_OUTPUT_DIR}' å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦ãã ã•ã„ã€‚")


# ---- å®Ÿè¡Œ ----
if __name__ == "__main__":
    # â˜…â˜…â˜… æœ¬ç•ªç”¨ã®è¨­å®šã«æˆ»ã—ã¾ã—ãŸ â˜…â˜…â˜…
    run_batch_generation(total_items=5000, batch_size=100)
