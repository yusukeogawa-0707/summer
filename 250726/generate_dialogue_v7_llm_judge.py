# generate_dialogues_large_scale_v8_ultimate.py

import json
import os
import random
import time
import re
from openai import OpenAI
from tqdm.notebook import tqdm

class Config:
    try:
        from google.colab import userdata
        API_KEY = userdata.get("OPENAI-KEY")
    except (ImportError, KeyError):
        API_KEY = os.environ.get("OPENAI_API_KEY", "YOUR_API_KEY_HERE")
    LLM_MODEL = "gpt-4o"
    PERSONA_DIR = "pilot_personas" # æœ¬ç•ªç”Ÿæˆæ™‚ã¯ "personas" ã«å¤‰æ›´
    OUTPUT_DIR = "pilot_dialogues" # æœ¬ç•ªç”Ÿæˆæ™‚ã¯ "dialogues" ã«å¤‰æ›´
    NUM_TURNS = 50
    NUM_INJECTIONS = 2
    PRESENCE_PENALTY = 0.2

def create_dynamic_injection_prompt(persona_data, fact_to_inject, dialogue_history):
    profile = persona_data["persona"]["profile"]
    persona_summary_text = "\n".join([f"- {key}: {value}" for key, value in profile.items()])
    history_text = "\n".join([f"{turn['speaker']}: {turn['content']}" for turn in dialogue_history[-10:]])
    core_fact_to_inject = f"- ã‚«ãƒ†ã‚´ãƒª: {fact_to_inject['category']}\n- å†…å®¹: {fact_to_inject['answer']}"
    user_prompt = f"""
# ã“ã‚Œã¾ã§ã®ä¼šè©±ã®æµã‚Œ
{history_text}
# ã‚ãªãŸã¸ã®ã‚¿ã‚¹ã‚¯
ä»¥ä¸‹ã®ãƒšãƒ«ã‚½ãƒŠã€ä¼šè©±ã®æµã‚Œã€ãã—ã¦é–‹ç¤ºã™ã¹ãæ ¸å¿ƒçš„äº‹å®Ÿã®ã™ã¹ã¦ã‚’è€ƒæ…®ã—ã€ã‚ãªãŸè‡ªèº«ã®è¨€è‘‰ã¨ã—ã¦ã€ã“ã®äº‹å®Ÿã‚’è£ä»˜ã‘ã‚‹å…·ä½“çš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å«ã‚“ã è‡ªç„¶ãªç™ºè©±ã‚’1ã¤ã ã‘ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
# ã‚ãªãŸãŒä»Šå›ã€è‡ªç„¶ã«è‡ªå·±é–‹ç¤ºã™ã‚‹ã¹ãã€Œæ ¸å¿ƒçš„ãªäº‹å®Ÿã€
{core_fact_to_inject}
# ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠæƒ…å ±ï¼ˆçµ¶å¯¾ã«å¿˜ã‚Œãªã„ã§ãã ã•ã„ï¼‰
{persona_summary_text}
# ã€â˜…â˜…é‡è¦ãƒ«ãƒ¼ãƒ«â˜…â˜…ã€‘
- ã“ã‚Œã¯ç¶™ç¶šã—ãŸä¼šè©±ã§ã™ã€‚é€”ä¸­ã§ã€Œã“ã‚“ã«ã¡ã¯ã€ã®ã‚ˆã†ãªæŒ¨æ‹¶ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚
"""
    return user_prompt

def is_utterance_consistent_with_llm(client, utterance, persona_profile):
    profile_text = "\n".join([f"- {key}: {value}" for key, value in persona_profile.items()])
    judge_prompt = f"""
ã‚ãªãŸã¯ã€äº‹å®Ÿã®çŸ›ç›¾ã‚’å³å¯†ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€é«˜æ€§èƒ½ãªåˆ¤å®šAIã§ã™ã€‚
# ãƒšãƒ«ã‚½ãƒŠã®ç¢ºå®šæƒ…å ±
{profile_text}
# åˆ¤å®šå¯¾è±¡ã®ç™ºè©±
ã€Œ{utterance}ã€
# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
ä¸Šè¨˜ã®ã€Œåˆ¤å®šå¯¾è±¡ã®ç™ºè©±ã€ãŒã€ã€Œãƒšãƒ«ã‚½ãƒŠã®ç¢ºå®šæƒ…å ±ã€ã¨æ˜ç¢ºã«çŸ›ç›¾ã™ã‚‹å†…å®¹ã‚’å«ã‚“ã§ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
åˆ¤å®šçµæœã‚’ã€å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{{
  "is_consistent": boolean,
  "reason": "çŸ›ç›¾ã—ã¦ã„ã‚‹ã€ã‚ã‚‹ã„ã¯ã—ã¦ã„ãªã„ã¨åˆ¤æ–­ã—ãŸç°¡æ½”ãªç†ç”±"
}}
"""
    try:
        response = client.chat.completions.create(
            model=Config.LLM_MODEL,
            messages=[{"role": "user", "content": judge_prompt}],
            response_format={"type": "json_object"},
            temperature=0.0,
        )
        judge_result = json.loads(response.choices[0].message.content)
        if not judge_result.get("is_consistent", True):
            print(f"\nâš ï¸ çŸ›ç›¾æ¤œçŸ¥(LLM-as-a-judge): {judge_result.get('reason')}")
        return judge_result.get("is_consistent", True)
    except Exception as e:
        print(f"  - åˆ¤å®šAPIã‚¨ãƒ©ãƒ¼: {e}")
        return True

def generate_dialogue_for_persona(persona_id):
    print(f"--- ãƒšãƒ«ã‚½ãƒŠID: {persona_id} ã®å¯¾è©±ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ ---")
    client = OpenAI(api_key=Config.API_KEY)
    persona_filepath = os.path.join(Config.PERSONA_DIR, f"persona_{persona_id:02d}.json")
    try:
        with open(persona_filepath, 'r', encoding='utf-8') as f:
            persona_data = json.load(f)
    except FileNotFoundError: return

    facts_to_inject = random.sample(persona_data["other_facts"], Config.NUM_INJECTIONS)
    injection_turns = sorted(random.sample(range(5, Config.NUM_TURNS - 5, 2), Config.NUM_INJECTIONS))
    injection_metadata = []
    dialogue_history = [{"speaker": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ"}]
    
    for turn_num in tqdm(range(1, Config.NUM_TURNS + 1), desc=f"ãƒšãƒ«ã‚½ãƒŠ {persona_id} å¯¾è©±ç”Ÿæˆä¸­"):
        current_role = "user" if turn_num % 2 != 0 else "assistant"
        prompt = ""
        temperature = 0.85
        
        if current_role == "user":
            if turn_num in injection_turns:
                fact_index = injection_turns.index(turn_num)
                fact_to_inject = facts_to_inject[fact_index]
                prompt = create_dynamic_injection_prompt(persona_data, fact_to_inject, dialogue_history)
                temperature = 0.9
                injection_metadata.append({
                    "injection_turn": turn_num, "qa_id": fact_to_inject.get('id', 'N/A'),
                    "category": fact_to_inject.get('category', 'N/A'), "core_fact_answer": fact_to_inject.get('answer', 'N/A')
                })
            else:
                profile_text = "\n".join([f"- {key}: {value}" for key, value in persona_data["persona"]["profile"].items()])
                prompt = f"""ç›´å‰ã®ç›¸æ‰‹ã®ç™ºè¨€ã€Œ{dialogue_history[-1]['content']}ã€ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒšãƒ«ã‚½ãƒŠã¨ã—ã¦è‡ªç„¶ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚\n# ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠæƒ…å ±ï¼ˆå½¹å‰²ï¼‰\n{profile_text}\n# ã€â˜…â˜…é‡è¦ãƒ«ãƒ¼ãƒ«â˜…â˜…ã€‘\n- ã“ã‚Œã¯ç¶™ç¶šã—ãŸä¼šè©±ã§ã™ã€‚é€”ä¸­ã§ã€Œã“ã‚“ã«ã¡ã¯ã€ã®ã‚ˆã†ãªæŒ¨æ‹¶ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚"""
        else:
            # â˜…â˜…â˜… ã‚ãªãŸã®ã€Œä¼šè©±æˆ¦ç•¥ã€ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«çµ±åˆ â˜…â˜…â˜…
            strategy = random.choice(["deepen", "deepen", "connect", "new_topic", "reflect"])
            instruction = ""
            if strategy == "deepen":
                instruction = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›´å‰ã®ç™ºè¨€å†…å®¹ã«ã¤ã„ã¦ã€ã•ã‚‰ã«ä¸€æ­©è¸ã¿è¾¼ã‚“ã ã‚ªãƒ¼ãƒ—ãƒ³ãªè³ªå•ã‚’æŠ•ã’ã‹ã‘ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªå·±é–‹ç¤ºã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚"
            elif strategy == "connect" and len(dialogue_history) > 10:
                user_utterances = [turn['content'] for turn in dialogue_history if turn['speaker'] == 'user']
                if user_utterances:
                    past_utterance = random.choice(user_utterances[:-1])
                    instruction = f"ä»¥å‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œ{past_utterance[:30]}...ã€ã¨è©±ã—ã¦ã„ãŸã“ã¨ã‚’æ€ã„å‡ºã—ã€ç¾åœ¨ã®è©±é¡Œã¨è‡ªç„¶ã«é–¢é€£ä»˜ã‘ã¦ä¼šè©±ã‚’åºƒã’ã¦ãã ã•ã„ã€‚"
                else:
                    strategy = "reflect" # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            elif strategy == "new_topic":
                instruction = "ç¾åœ¨ã®è©±é¡ŒãŒä¸€æ®µè½ã—ãŸã¨åˆ¤æ–­ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèˆˆå‘³ã‚’æŒã¡ãã†ãªã€ã“ã‚Œã¾ã§è©±ã—ã¦ã„ãªã„å…¨ãæ–°ã—ã„é›‘è«‡ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’è‡ªç„¶ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
            
            if not instruction or strategy == "reflect":
                instruction = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€å†…å®¹ã‚’è¦ç´„ã—ã€ãã®å†…å®¹ã«å¯¾ã™ã‚‹ã‚ãªãŸã®ç†è§£ã‚„å…±æ„Ÿã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚"

            prompt = f"""ã‚ãªãŸã¯èãä¸Šæ‰‹ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã€Œ{dialogue_history[-1]['content']}ã€ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æˆ¦ç•¥ã«å¾“ã£ã¦ã€è¦ªèº«ã«ã€ã‹ã¤è‡ªç„¶ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚\n# ä¼šè©±æˆ¦ç•¥\n{instruction}\n# ã€â˜…â˜…é‡è¦ãƒ«ãƒ¼ãƒ«â˜…â˜…ã€‘\n- ã“ã‚Œã¯ç¶™ç¶šã—ãŸä¼šè©±ã§ã™ã€‚é€”ä¸­ã§ã€Œã“ã‚“ã«ã¡ã¯ã€ã®ã‚ˆã†ãªæŒ¨æ‹¶ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚\n- ä¼šè©±ã‚’ç· ã‚ããã‚‹ã‚ˆã†ãªç™ºè¨€ï¼ˆã€Œä½•ã‹ä»–ã«ã‚ã‚Šã¾ã™ã‹ï¼Ÿã€ãªã©ï¼‰ã¯é¿ã‘ã€å¸¸ã«å¯¾è©±ãŒç¶šãã‚ˆã†ãªã‚ªãƒ¼ãƒ—ãƒ³ãªå¿œç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""

        max_retries = 3
        utterance = ""
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=Config.LLM_MODEL, messages=[{"role": "user", "content": prompt}],
                    temperature=temperature, presence_penalty=Config.PRESENCE_PENALTY
                )
                temp_utterance = response.choices[0].message.content.strip()
                if current_role == "user":
                    if is_utterance_consistent_with_llm(client, temp_utterance, persona_data["persona"]["profile"]):
                        utterance = temp_utterance
                        break
                    else:
                        if attempt == max_retries - 1:
                           print(f"\nâŒ å†ç”Ÿæˆãƒªãƒˆãƒ©ã‚¤ä¸Šé™åˆ°é”ã€‚")
                           utterance = temp_utterance
                else:
                    utterance = temp_utterance
                    break
            except Exception as e:
                utterance = "(ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šç™ºè©±ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ)"
                break
        dialogue_history.append({"speaker": current_role, "content": utterance})
        time.sleep(1)

    if not os.path.exists(Config.OUTPUT_DIR): os.makedirs(Config.OUTPUT_DIR)
    output_filename = os.path.join(Config.OUTPUT_DIR, f"dialogue_p{persona_id:02d}.json")
    with open(output_filename, 'w', encoding='utf-8') as f: json.dump(dialogue_history, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… å¯¾è©±ç”ŸæˆãŒå®Œäº†ã—ã€'{output_filename}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    metadata_filename = os.path.join(Config.OUTPUT_DIR, f"dialogue_p{persona_id:02d}.metadata.json")
    with open(metadata_filename, 'w', encoding='utf-8') as f: json.dump(injection_metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… æ³¨å…¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{metadata_filename}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    START_PERSONA_ID = 1
    END_PERSONA_ID = 100 

    print(f"ã€INFOã€‘ãƒšãƒ«ã‚½ãƒŠID {START_PERSONA_ID} ã‹ã‚‰ {END_PERSONA_ID} ã¾ã§ã®å¤§è¦æ¨¡å¯¾è©±ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    for i in range(START_PERSONA_ID, END_PERSONA_ID + 1):
        print("\n" + "="*50)
        generate_dialogue_for_persona(persona_id=i)
        print("="*50)
        if i < END_PERSONA_ID:
            print(f"\nãƒšãƒ«ã‚½ãƒŠ {i} ã®ç”Ÿæˆå®Œäº†ã€‚æ¬¡ã®ãƒšãƒ«ã‚½ãƒŠã«é€²ã‚€å‰ã«30ç§’å¾…æ©Ÿã—ã¾ã™...")
            time.sleep(30)

    print("\nğŸ‰ å…¨ã¦ã®ãƒšãƒ«ã‚½ãƒŠã®å¯¾è©±ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
